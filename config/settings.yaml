logger:
  dir_logger: data/logs/app.log  # Directory for logs
  console_log: true
  file_log: true
  log_level: INFO  # CRITICAL FATAL ERROR WARN WARNING INFO DEBUG NOTSET
  logger_name: app-log

# Model Configuration
llm:
  model_type: "llama" # openai, mistral, llama
  open_ai_model: "gpt-3.5-turbo-0125" #Only for openai model
  env: "/export/usuarios_ml4ds/lbartolome/Repos/repos_con_carlos/RAG_tool/.env"

# ACRONYM Configuration
acr:
  data_column_name: "raw_text_x"  # Column name to read, can be 'objective','text', 'title', 'texto_sin_preprocesar'->PLACE
  train_all_modules: false  # If is true, all modules will be trained, if is false, only the modules in the list modules_to_train will be trained
  modules_to_train:  # List of modules to train
    #- HermesAcronymExpander
    #- HermesAcronymDetector

# Preprocessing Configuration
preproc:
  preprocessing_script: "/export/usuarios_ml4ds/cggamella/RAG_tool/src/preprocessing/pipe/nlpipe.py"
  source_type : "parquet"
  spacy_model : "en_core_web_lg" #en_core_web_lg, es_core_news_lg
  lang : "en" # "en" or "es"
  do_lemmatization: true #New parameter to disable lemmatization in the preprocessing

equiv:
  path_results: "/export/usuarios_ml4ds/cggamella/RAG_tool/data/FINAL_MODELS/"
  num_topics_equiv : 100 # It's recommended to use 100 or any big number to find more equivalences
  further_proc: True
  model_name: "MalletLda"
  top_k : 100 