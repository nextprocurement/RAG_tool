import argparse
import pathlib
import time
import yaml
import subprocess
import shutil
import json
from src.acronyms.acronym_detector import HermesAcronymDetector
from src.acronyms.acronym_expander import HermesAcronymExpander
from src.equivalences.equivalences_generator import HermesEquivalencesGenerator
from src.utils.tm_utils import train_model
from src.utils.utils import (init_logger, process_dataframe, generate_acronym_expansion_json)

def load_config(config_path):
    """
    Load configuration from YAML.
    """
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)
    return config

def main():
    ###############################################
    #               GENERAL ARGUMENTS             #  
    ###############################################
    parser = argparse.ArgumentParser(description="HERMES pipeline")
    parser.add_argument(
        "--llm_type",
        type=str,
        default="llama",
        help="Type of large language model to use (llama, openai, mistral...)") 
    parser.add_argument(
        "--data_path",
        type=str,
        help="Path to data file",
        default='/export/usuarios_ml4ds/cggamella/RAG_tool/files/anotacion_manual/fam/datos_modelo_es_Mallet_df_merged_14_topics_45_ENTREGABLE.parquet')
    parser.add_argument(
        "--save_path",
        type=str,
        help="Path to save the output files",
        default="/export/usuarios_ml4ds/cggamella/RAG_tool/data/integracion_STOPS/out")
    parser.add_argument(
        "--mode",
        type=str,
        default="optimized",
        help="Mode of operation (optimized or non-optimized)")

    ###############################################
    # ACRONYM DETECTION AND EXPANSION ARGUMENTS   #
    ###############################################
    parser.add_argument(
        "--do_train",
        action='store_true',
        help="Indicate if the models should be trained.")
    parser.add_argument(
        "--train_data_path",
        type=str,
        default=None,
        help="Path to training DSPy modules for the acronym detector and expander"
    )
    parser.add_argument(
        "--context_window",
        type=int,
        default=3000,
        help="Size of the context window for the chunker")
    parser.add_argument(
        "--max_windows", 
        type=int,
        default=100,
        help="Maximum number of windows the chunker can generate")
    parser.add_argument(
        "--window_overlap",
        type=float,
        default=0.1,
        help="Percentage of overlap between windows generated by the chunker")
    
    ###############################################
    #           PREPROCESSING ARGUMENTS           #
    ###############################################
    parser.add_argument(
        "--preproc_source",
        type=str,
        default="cpv45",
        help="Source of the data in the preproc file (config.json)")

    ###############################################
    #         EQUIVALENCE DETECTION ARGUMENTS     #
    ###############################################
    parser.add_argument(
        "--source_eq",
        action='store_true',
        default="tm",
        help="Source of the equivalences (vocabulary or tm)")
    parser.add_argument(
        "--times_equiv",
        action='store_true',
        default=1, # For final models fix to 3
        help="Number of times to run the equivalence detection"
    )
        
    ###############################################
    #               TRAINING ARGUMENTS            #
    ###############################################
    parser.add_argument(
        "--num_topics",
        help="Number of topics",
        type=str, default="5,10,15,20,30", required=False)
    parser.add_argument(
        "--num_iters",
        help="Number of iterations",
        type=int, default=1000, required=False)
    parser.add_argument(
        "--model_type",
        help="type of the model, MalletLda, Ctm, BERTopic, all",
        type=str, default='MalletLda', required=False)
    parser.add_argument(
        "--sample",
        help="how many documents to run",
        type=int, 
        required=False,
        default=100
    )
    parser.add_argument(
        "--do_second_level",
        help="Whether to generate second-level topics.",
        type=bool, 
        default=True
    )
    parser.add_argument(
        "--further_proc",
        help="Whether to further process the data.",
        type=bool, 
        default=True
    )
    
    # ***********************************************************************
    # 0. Setup
    # ***********************************************************************
    config_path = pathlib.Path("config/settings.yaml")
    config = load_config(config_path)
    logger = init_logger(config['logger'])
    args = parser.parse_args()

    
    if args.mode == "optimized":
        logger.info(f"-- -- Running HERMES pipeline in optimized mode...")
        path_root_save = pathlib.Path(args.save_path) / 'optimized'
        #path_root_save_checkpoint = pathlib.Path(args.save_path) / 'checkpoints'
        model_name = config.get("llm", {}).get("model_type", "default_model")
        # AÃ±adir _nombre_modelo a la ruta
        path_root_save = path_root_save / f"{model_name}"
        if not path_root_save.exists():
            path_root_save.mkdir(parents=True)
            
        #**********************************************************************
        # 1. Acronym Detection and Expansion
        #**********************************************************************
        logger.info("*"*150)
        logger.info("### HERMES pipeline ###")
        logger.info("*"*150)
        logger.info("#"*80)
        logger.info(f"### 1. Acronym Detection and Expansion ###")
        logger.info("#"*80)
        
        # Check if this step has been carried out already for the input data 
        path_save = path_root_save / '1.acronym_detection_expansion'
        if not path_save.exists():
            path_save.mkdir(parents=True)
        file_save = pathlib.Path(args.data_path).stem + '.parquet'
        path_save = path_save / file_save
        
        if path_save.exists():
            logger.info(f"-- -- Acronym Detection and Expansion output already exists at {path_save.as_posix()}")
        else:
            logger.info(f"-- -- Acronym Detection and Expansion output does not exist at {path_save.as_posix()}")
            logger.info(f"-- -- Running Acronym Detection and Expansion...")
            time_start = time.time()
        
            #############
            # DETECTION #
            #############
            # Decide whether to train based on configuration file and arguments
            do_train = getattr(args, 'do_train', False) or config['acr'].get('train_all_modules', False)
            
            train_data_path = args.train_data_path or config['acr'].get('train_data_path')

            if do_train and not train_data_path:
                logger.error("Training data path is required for training but not provided.")
                return
            
            # Verify if the model should be trained
            should_train_detector = do_train or "HermesAcronymDetector" in (config['acr'].get('modules_to_train') or [])
            
            if should_train_detector:
                detector = HermesAcronymDetector(
                    model_type=args.llm_type,
                    do_train=should_train_detector,
                    data_path=train_data_path,  # Usar train_data_path para entrenamiento
                    logger=logger
                )
            else:
                detector = HermesAcronymDetector(
                    model_type=args.llm_type,
                    do_train=should_train_detector,
                    logger=logger
                )
            
            #############
            # EXPANSION #
            #############
            # Verify if the Expander should be trained or not
            should_train_expander = do_train or "HermesAcronymExpander" in (config['acr'].get('modules_to_train') or [])
            
            if should_train_expander:
                expander = HermesAcronymExpander(
                    model_type=args.llm_type,
                    do_train=should_train_expander,
                    data_path=train_data_path,  # Usar train_data_path para entrenamiento
                    logger=logger
                )
            else:
                expander = HermesAcronymExpander(
                    model_type=args.llm_type,
                    do_train=should_train_expander,
                    logger=logger
                )
            
            # Process the DataFrame for detection and expansion
            try:
                df_out = process_dataframe(
                    path=args.data_path,
                    config=config['acr'],
                    action="both",
                    acronym_detector=detector.module if detector else None,
                    acronym_expander=expander.module if expander else None,
                    context_window=args.context_window,
                    max_windows=args.max_windows,
                    window_overlap=args.window_overlap,
                    logger=logger
                )
            except Exception as e:
                logger.error(f"-- -- Error occured processing dataframe: {str(e)}")
                raise e
            
            logger.info(f"-- -- Acronym Detection and Expansion done in {(time.time() - time_start)/60} minutes. Saving output...")
            df_out.to_parquet(path_save, index=False)
            logger.info(f"-- -- 1. Acronym Detection and Expansion output saved to {path_save.as_posix()}")  
            logger.info(f"-- -- Generating acronym expansion JSON...")
            #path_sal = '/export/usuarios_ml4ds/cggamella/RAG_tool/src/topicmodeling/data/acronyms/'
            path_save_acronym_expansion = path_root_save / '1.acronym_detection_expansion'
            generate_acronym_expansion_json(path_save, path_save_acronym_expansion)
            logger.info(f"JSON with detected and expanded acronyms saved on {path_save_acronym_expansion}")
                
        #***********************************************************************
        # 2. Preprocessing
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 2. Preprocessing ###")
        logger.info("#"*80)
        
        source_path = path_save # Path to the output file from the previous step
        path_save = path_root_save / '2.preprocessing'
        if not path_save.exists():
            path_save.mkdir(parents=True)
        file_save = pathlib.Path(args.data_path).stem + '.parquet'    
        path_save = path_save / file_save
        
        if path_save.exists():
            logger.info(f"-- Preprocessing output already exists at {path_save.as_posix()}")
        else:
            logger.info(f"-- Preprocessing output does not exist at {path_save.as_posix()}")
            logger.info(f"-- Running Preprocessing...")
            time_start = time.time()
            
            do_lemmatization = config['preproc'].get('do_lemmatization', True)
            if do_lemmatization:
                logger.info("-- Lemmatization is enabled. --")
            else:
                logger.info("-- Lemmatization is disabled. --")
            
            cmd = [
                "python", (config['preproc'].get('preprocessing_script')),
                "--source_path", source_path.as_posix(),
                "--source_type", (config['preproc'].get('source_type')),
                "--source", args.preproc_source,
                "--destination_path", path_save.as_posix(),
                "--lang", (config['preproc'].get('lang')),
                "--spacy_model", (config['preproc'].get('spacy_model')),
                "--do_embeddings"
            ]
             
            if not do_lemmatization:
                cmd.append('--no_lemmatization')
            try:
                logger.info(f'-- -- Running preprocessing command {" ".join(cmd)}')
                subprocess.check_output(cmd)
            except subprocess.CalledProcessError as e:
                logger.info('-- -- Preprocessing failed. Revise command')
                logger.info(e.output)            
            logger.info(f"-- -- Preprocessing done in {(time.time() - time_start)/60} minutes. Saving output...")
            logger.info(f"-- -- 2. Preprocessing output saved to {path_save.as_posix()}")
            
        #***********************************************************************
        # 3. Equivalence Detection
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 3. Equivalence Detection ###")
        logger.info("#"*80)
        
        load_data_path = path_save.parent / (path_save.stem + "_embeddings.parquet") # Path to the output file from the previous step
        path_save = path_root_save / '3.equivalence_detection'
        if not path_save.exists():
            path_save.mkdir(parents=True)
            
        path_save_eqs = path_save / 'equivalences_lst'
        
        if not path_save_eqs.exists():
            path_save_eqs.mkdir(parents=True)
            print(f"Directorio creado: {path_save_eqs}")
        
        path_origen = path_root_save / '1.acronym_detection_expansion'
        archivos_json = list(path_origen.glob("*.json"))
        
        if not archivos_json:
            print(f"No se encontraron archivos .json en: {path_origen}")
            return

        for archivo in archivos_json:
            try:
                destino = path_save_eqs
                # Copiar el archivo
                shutil.copy(archivo, destino)
                print(f"Copiado: {archivo} --> {destino}")
            except Exception as e:
                print(f"Error al copiar {archivo}: {e}")
              
        file_save = pathlib.Path(args.data_path).stem + '.json'
        path_save_eqs_file = path_save_eqs / file_save

        eq_generator = HermesEquivalencesGenerator(
            model_type = config['llm']['model_type'],
            use_optimized = True,
            do_train = True,
            lang=config['preproc']['lang']
        )
    
        if args.source_eq == "vocabulary":
            
            this_path_save_eqs = path_save_eqs_file.parent / f"{path_save_eqs_file.stem}_vocabulary.json"
            
            if this_path_save_eqs.exists():
                logger.info(f"-- -- Equivalences output already exists at {this_path_save_eqs.as_posix()}")
            else:
                logger.info(f"-- -- Equivalences output does not exist at {this_path_save_eqs.as_posix()}")
                logger.info(f"-- -- Running Equivalence Detection...")
                time_start = time.time()    
            
            # Train auxiliary topic model
            model_path = path_save / pathlib.Path(args.data_path).stem /'aux_topic_model_vocabulary'
            
            if model_path.exists():
                logger.info(f"-- -- Auxiliary topic model already exists at {model_path}. Using it...")
            else:
                model_path.mkdir(parents=True)
                logger.info(f"-- -- Training auxiliary topic model for vocabulary...")

                this_args = argparse.Namespace(
                    **{k: v for k, v in vars(args).items() 
                    if v is not None and k in ["further_proc", "sample", "num_iters"]})

                # Assign new values to the copied Namespace object
                this_args.model_path = model_path.as_posix()
                this_args.load_data_path = load_data_path.as_posix()
                this_args.num_topics = config['equiv']['num_topics_equiv']
                this_args.logger = logger
                                
                model = train_model(
                    model_path = model_path.as_posix(),
                    model_type = config['equiv']['model_name'],
                    num_topics = config['equiv']['num_topics_equiv'],
                    further_proc = config['equiv']['further_proc'],
                    logger = logger,
                    env = pathlib.Path(config['llm']['env']),
                    args = this_args,
                    stw_path = '/export/usuarios_ml4ds/cggamella/RAG_tool/data/FINAL_MODELS/stops',
                    eq_path = path_save_eqs_file
                )
                topics = model.print_topics()
                print(f"-- -- Topics from auxiliary trained model: {topics}")
                for i, topic in enumerate(topics):
                    print("Topic #", i)
                    print(topics[topic])
            
            if args.source_eq == "vocabulary":
                path_to_source = (
                    model_path /
                    f"{config['equiv']['model_name']}_{config['equiv']['num_topics_equiv']}" /
                    "vocabulary.txt"
                )
            else:
                path_to_source = (
                    model_path /
                    f"{config['equiv']['model_name']}_{config['equiv']['num_topics_equiv']}"
                ) 
            
            try:
                logger.info(f"-- -- Generating equivalences from {args.source_eq}...") 

                json_data_old, json_data_new, stats = eq_generator.generate_equivalences(
                    source=args.source_eq,
                    path_to_source=path_to_source,
                    path_save=this_path_save_eqs,
                    model_type=config['equiv']['model_name'],
                    language=config['preproc']['lang'],
                    top_k=config['equiv']['top_k'],
                )
                
                # Paths to save the JSON files
                path_save_old = path_save_eqs.with_name(f"{path_save_eqs.stem}_old{path_save_eqs.suffix}.json")
                path_save_new = path_save_eqs.with_name(f"{path_save_eqs.stem}_new{path_save_eqs.suffix}.json")
                path_save_stats = path_save_eqs.with_name(f"{path_save_eqs.stem}_stats.json")

                with open(path_save_old, 'w', encoding='utf-8') as json_file:
                    json.dump(json_data_old, json_file, indent=4, ensure_ascii=False)
                logger.info(f"-- -- Old equivalences saved to {path_save_old}")
                with open(path_save_new, 'w', encoding='utf-8') as json_file:
                    json.dump(json_data_new, json_file, indent=4, ensure_ascii=False)
                logger.info(f"-- -- New equivalences saved to {path_save_new}")
                
                # Save statistics of clustering in a JSON file
                with open(path_save_stats, 'w', encoding='utf-8') as json_file:
                    json.dump(stats, json_file, indent=4, ensure_ascii=False)
                logger.info(f"-- -- Paper equivalences statistics saved to {path_save_stats}")
                logger.info(f"-- -- Equivalences generation completed successfully.")

            except Exception as e:
                logger.error(f"Error during equivalence generation or saving: {e}")

            logger.info(f"-- -- Equivalences saved to {this_path_save_eqs}")
        
        elif args.source_eq == "tm":
            
            logger.info(f"-- -- Running equivalence detection for TM {args.times_equiv} times...")
        
            for t in range(args.times_equiv):
                
                this_path_save_eqs = path_save_eqs.parent / f"{path_save_eqs.stem}_{t+1}.json"
                
                if not this_path_save_eqs.exists():
                    
                    logger.info(f"-- -- Running equivalence detection {t+1} out of {args.times_equiv}...")
            
                    # Train auxiliary topic model
                    model_path = path_save / pathlib.Path(args.data_path).stem /f'aux_topic_model_tm_{t+1}'
                    
                    if model_path.exists():
                        logger.info(f"-- -- Auxiliary topic model already exists at {model_path}. Using it...")
                    else:
                        model_path.mkdir(parents=True)
                        logger.info(f"-- -- Training auxiliary topic model for TM {t+1}...")

                        this_args = argparse.Namespace(
                            **{k: v for k, v in vars(args).items() 
                            if v is not None and k in ["further_proc", "sample", "num_iters"]})

                        # Assign new values to the copied Namespace object
                        this_args.model_path = model_path.as_posix()
                        this_args.load_data_path = load_data_path.as_posix()
                        this_args.num_topics = config['equiv']['num_topics_equiv']
                        this_args.logger = logger
                                        
                        model = train_model(
                            model_path = model_path.as_posix(),
                            model_type = config['equiv']['model_name'],
                            num_topics = config['equiv']['num_topics_equiv'],
                            further_proc = config['equiv']['further_proc'],
                            logger = logger,
                            env = pathlib.Path(config['llm']['env']),
                            args = this_args,
                            stw_path = '/export/usuarios_ml4ds/cggamella/RAG_tool/data/FINAL_MODELS/stops',
                            eq_path = path_save_eqs
                        )
                        topics = model.print_topics()
                        print(f"-- -- Topics from auxiliary trained model: {topics}")
                        for i, topic in enumerate(topics):
                            print("Topic #", i)
                            print(topics[topic])
                    
                    path_to_source = model_path / f"{config['equiv']['model_name']}_{config['equiv']['num_topics_equiv']}" / "vocabulary.txt" if args.source_eq == "vocabulary" else model_path / f"{config['equiv']['model_name']}_{config['equiv']['num_topics_equiv']}"

                    try:
                        logger.info(f"-- -- Generating equivalences from {args.source_eq}...") 

                        json_data_old, json_data_new, stats = eq_generator.generate_equivalences(
                            source=args.source_eq,
                            path_to_source=path_to_source,
                            path_save=this_path_save_eqs,
                            model_type=config['equiv']['model_name'],
                            language=config['preproc']['lang'],
                            top_k=config['equiv']['top_k'],
                        )
                        
                        # Paths to save the JSON files
                        path_save_old = path_save_eqs / f"{path_save_eqs.stem}_old.json"
                        path_save_new = path_save_eqs / f"{path_save_eqs.stem}_new.json"         
                        path_save_stats = path_save_eqs.parent / f"{path_save_eqs.stem}_stats.json"
                        
                        with open(path_save_old, 'w', encoding='utf-8') as json_file:
                            json.dump(json_data_old, json_file, indent=4, ensure_ascii=False)
                        logger.info(f"-- -- Old equivalences saved to {path_save_old}")
                        with open(path_save_new, 'w', encoding='utf-8') as json_file:
                            json.dump(json_data_new, json_file, indent=4, ensure_ascii=False)
                        logger.info(f"-- -- New equivalences saved to {path_save_new}")
                        
                        # Save statistics of clustering in a JSON file
                        with open(path_save_stats, 'w', encoding='utf-8') as json_file:
                            json.dump(stats, json_file, indent=4, ensure_ascii=False)
                        logger.info(f"-- -- Paper equivalences statistics saved to {path_save_stats}")
                        logger.info(f"-- -- Equivalences generation completed successfully.")

                    except Exception as e:
                        logger.error(f"Error during equivalence generation or saving: {e}")

        else:
            logger.error(f"-- -- Source of equivalences not recognized. Please provide a valid source.")
            return
        
        #**********************************************************************
        # 4. Training
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 4. Training ###")
        logger.info("#"*80)
        
        path_save = path_root_save / '4.training'
        if not path_save.exists():
            path_save.mkdir(parents=True)
            
        model_path = path_save / pathlib.Path(args.data_path).stem
        if not model_path.exists():
            model_path.mkdir(parents=True)
            logger.info(f"-- --Creating {model_path.as_posix()} ...")       
     
        try:
            num_topics_lst = args.num_topics.split(",")
            num_topics_lst = [int(num) for num in num_topics_lst]
        except Exception as e:
            logger.info(f"-- -- Error splitting num_topics: {str(e)}. Number of topics is not a list.")
            num_topics_lst = [int(args.num_topics)]
        
        for num_topics in num_topics_lst:
            this_args = argparse.Namespace(
                **{k: v for k, v in vars(args).items() 
                if v is not None and k in ["further_proc", "sample", "num_iters"]})

            # Assign new values to the copied Namespace object
            this_args.model_path = model_path.as_posix()
            this_args.load_data_path = load_data_path.as_posix()
            this_args.num_topics = num_topics#args.num_topics
            this_args.logger = logger
            
            if args.model_type == 'all':
                logger.info( "-- -- Training all models...")
                models = ['MalletLda', 'Ctm', 'BERTopic', 'TopicGPT']
            else:
                logger.info( f"-- -- Training model of type {args.model_type}...")
                models = [args.model_type]
            
            for model_type in models:
                
                #model_path_complete = model_path / f"{model_type}_{num_topics}"
                
                #if model_path_complete.exists():
                #    logger.info(f"-- -- Model output already exists at {model_path_complete}")
                #else:
                #    logger.info(f"-- -- Model output does not exist at {model_path}. Training model...")
                
                    if model_type == 'Ctm':
                        this_args.num_iters = 50
                        logger.info(f"-- -- Training model with {this_args.num_iters} iterationss because it is a Ctm model...")
                    else:
                        this_args.num_iters = args.num_iters
                        
                    model = train_model(
                        model_path = model_path.as_posix(),
                        model_type = model_type,
                        num_topics = num_topics,
                        further_proc = args.further_proc,
                        logger = logger,
                        env = pathlib.Path(config['llm']['env']),
                        args = this_args,
                        stw_path = '/export/usuarios_ml4ds/cggamella/RAG_tool/data/FINAL_MODELS/stops',
                        eq_path = path_save_eqs
                    )
                    topics = model.print_topics()
                    print(f"-- -- Topics from auxiliary trained model: {topics}")
                    for i, topic in enumerate(topics):
                        print("Topic #", i)
                        print(topics[topic])
    
    else:
        logger.info("-- -- Running HERMES pipeline in non-optimized mode...")
        
        path_root_save = pathlib.Path(args.save_path) / 'non_optimized'
        if not path_root_save.exists():
            path_root_save.mkdir(parents=True)
            
        #***********************************************************************
        # 2. Preprocessing
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 2. Preprocessing ###")
        logger.info("#"*80)
        
        source_path = args.data_path
        path_save = path_root_save / '2.preprocessing'
        if not path_save.exists():
            path_save.mkdir(parents=True)
        file_save = pathlib.Path(args.data_path).stem + '.parquet'    
        path_save = path_save / file_save
        
        if path_save.exists():
            logger.info(f"-- -- Preprocessing output already exists at {path_save.as_posix()}")
        else:
            logger.info(f"-- -- Preprocessing output does not exist at {path_save.as_posix()}")
            logger.info(f"-- -- Running Preprocessing...")
            time_start = time.time()
            
            do_lemmatization = config['preproc'].get('do_lemmatization', True)
            
            if do_lemmatization:
                logger.info("-- -- Lemmatization is enabled.")
            else:
                logger.info("-- -- Lemmatization is disabled.")
        
            cmd = [
                "python", (config['preproc'].get('preprocessing_script')),
                "--source_path", source_path,
                "--source_type", (config['preproc'].get('source_type')),
                "--source", args.preproc_source + "_non_optimized",
                "--destination_path", path_save.as_posix(),
                "--lang", (config['preproc'].get('lang')),
                "--spacy_model", (config['preproc'].get('spacy_model')),
                "--do_embeddings"
            ]
            if not do_lemmatization:
                cmd.append('--no_lemmatization')

            try:
                logger.info(f'-- -- Running preprocessing command {" ".join(cmd)}')
                subprocess.check_output(cmd)
            except subprocess.CalledProcessError as e:
                logger.info('-- -- Preprocessing failed. Revise command')
                logger.info(e.output)
            
            logger.info(f"-- -- Preprocessing done in {(time.time() - time_start)/60} minutes. Saving output...")
            logger.info(f"-- -- 2. Preprocessing output saved to {path_save.as_posix()}")
    
        #**********************************************************************
        # 4. Training
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 4. Training ###")
        logger.info("#"*80)
        
        load_data_path = path_save.parent / (path_save.stem + "_embeddings.parquet") # Path to the output file from the previous step
        
        path_save = path_root_save / '4.training'
        if not path_save.exists():
            path_save.mkdir(parents=True)
            
        model_path = path_save / pathlib.Path(args.data_path).stem
        
        model_path = path_save / pathlib.Path(args.data_path).stem
        if not model_path_complete.exists():
            model_path.mkdir(parents=True)
            logger.info(f"-- --Creating {model_path.as_posix()} ...")       
     
        try:
            num_topics_lst = args.num_topics.split(",")
            num_topics_lst = [int(num) for num in num_topics_lst]
        except Exception as e:
            logger.info(f"-- -- Error splitting num_topics: {str(e)}. Number of topics is not a list.")
            num_topics_lst = [int(args.num_topics)]
        
        for num_topics in num_topics_lst:
            
            this_args = argparse.Namespace(
                **{k: v for k, v in vars(args).items() 
                if v is not None and k in ["sample", "num_iters"]})

            # Assign new values to the copied Namespace object
            this_args.model_path = model_path.as_posix()
            this_args.load_data_path = load_data_path.as_posix()
            this_args.further_proc = False
            this_args.num_topics = num_topics #args.num_topics
            this_args.logger = logger
            
            if args.model_type == 'all':
                logger.info( "-- -- Training all models...")
                models = ['MalletLda', 'Ctm', 'BERTopic', 'TopicGPT']
            else:
                logger.info( f"-- -- Training model of type {args.model_type}...")
                models = [args.model_type]
            
            for model_type in models:
                
                model_path_complete = model_path / f"{model_type}_{num_topics}"
                
                if model_path_complete.exists():
                    logger.info(f"-- -- Model output already exists at {model_path_complete}")
                else:
                    model_path.mkdir(parents=True)
                    logger.info(f"-- -- Model output does not exist at {model_path}. Training model...")
                
                    if model_type == 'Ctm':
                        this_args.num_iters = 50
                        logger.info(f"-- -- Training model with {this_args.num_iters} iterationss because it is a Ctm model...")
                    else:
                        this_args.num_iters = args.num_iters
                        
                    model = train_model(
                        model_path = model_path.as_posix(),
                        model_type = model_type,
                        num_topics = num_topics,
                        further_proc = False,
                        logger = logger,
                        env = pathlib.Path(config['llm']['env']),
                        args = this_args
                    )
                    topics = model.print_topics()
                    print(f"-- -- Topics from auxiliary trained model: {topics}")
                    for i, topic in enumerate(topics):
                        print("Topic #", i)
                        print(topics[topic])

if __name__ == "__main__":
    main()
    
    
