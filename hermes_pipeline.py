import argparse
import pathlib
import time
import yaml
import subprocess
import shutil
from src.acronyms.acronym_detector import HermesAcronymDetector
from src.acronyms.acronym_expander import HermesAcronymExpander
from src.equivalences.equivalences_generator import HermesEquivalencesGenerator
from src.utils.tm_utils import train_model
from src.utils.utils import (init_logger, process_dataframe, generate_acronym_expansion_json)

def load_config(config_path):
    """
    Load configuration from YAML.
    """
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)
    return config

def main():
    # ***********************************************************************
    # Parse arguments
    # ***********************************************************************
    ###############################################
    # GENERAL ARGUMENTS                           #  
    ###############################################
    parser = argparse.ArgumentParser(description="HERMES pipeline")
    parser.add_argument(
        "--llm_type",
        type=str,
        default="llama",
        help="Type of large language model to use (llama or openai)") 
    parser.add_argument(
        "--data_path",
        type=str,
        help="Path to data file",
        default='/export/usuarios_ml4ds/cggamella/RAG_tool/files/anotacion_manual/fam/datos_modelo_es_Mallet_df_merged_14_topics_45_ENTREGABLE.parquet')
    parser.add_argument(
        "--save_path",
        type=str,
        help="Path to save the output files",
        default="/export/usuarios_ml4ds/lbartolome/Repos/repos_con_carlos/RAG_tool/data/out")
    parser.add_argument(
        "--mode",
        type=str,
        default="optimized",
        help="Mode of operation (optimized or non-optimized)")

    ###############################################
    # ACRONYM DETECTION AND EXPANSION ARGUMENTS   #
    ###############################################
    parser.add_argument(
        "--do_train",
        action='store_true',
        help="Indicate if the models should be trained.")
    parser.add_argument(
        "--context_window",
        type=int,
        default=3000,
        help="Size of the context window for the chunker")
    parser.add_argument(
        "--max_windows", 
        type=int,
        default=100,
        help="Maximum number of windows the chunker can generate")
    parser.add_argument(
        "--window_overlap",
        type=float,
        default=0.1,
        help="Percentage of overlap between windows generated by the chunker")
    
    ###############################################
    # PREPROCESSING ARGUMENTS                     #
    ###############################################
    parser.add_argument(
        "--preproc_source",
        type=str,
        default="cpv45",
        help="Source of the data in the preproc file (config.json)")
    
    ###############################################
    # EQUIVALENCE DETECTION ARGUMENTS             #
    ###############################################
    parser.add_argument(
        "--source_eq",
        action='store_true',
        default="vocabulary",
        help="Source of the equivalences (vocabulary or tm)")
        
    ###############################################
    # TRAINING ARGUMENTS                          #
    ###############################################
    parser.add_argument(
        "--num_topics",
        help="Number of topics",
        type=int, default=14, required=False)
    parser.add_argument(
        "--num_iters",
        help="Number of iterations",
        type=int, default=1000, required=False)
    parser.add_argument(
        "--model_type",
        help="type of the model, MalletLda, Ctm, BERTopic, all",
        type=str, default='all', required=False)
    parser.add_argument(
        "--sample",
        help="how many documents to run",
        type=int, 
        required=False,
        default=100
    )
    parser.add_argument(
        "--do_second_level",
        help="Whether to generate second-level topics.",
        type=bool, 
        default=True
    )
    parser.add_argument(
        "--further_proc",
        help="Whether to further process the data.",
        type=bool, 
        default=True
    )
    
    # ***********************************************************************
    # 0. Setup
    # ***********************************************************************
    config_path = pathlib.Path("config/settings.yaml")
    config = load_config(config_path)
    logger = init_logger(config['logger'])
    args = parser.parse_args()

    if args.mode == "optimized":
        logger.info(f"-- -- Running HERMES pipeline in optimized mode...")
        path_root_save = pathlib.Path(args.save_path) / 'optimized'
        if not path_root_save.exists():
            path_root_save.mkdir(parents=True)

        #**********************************************************************
        # 1. Acronym Detection and Expansion
        #**********************************************************************
        logger.info("*"*150)
        logger.info("### HERMES pipeline ###")
        logger.info("*"*150)
        logger.info("#"*80)
        logger.info(f"### 1. Acronym Detection and Expansion ###")
        logger.info("#"*80)
        
        # Check if this step has been carried out already for the input data 
        path_save = path_root_save / '1.acronym_detection_expansion'
        if not path_save.exists():
            path_save.mkdir(parents=True)
        file_save = pathlib.Path(args.data_path).stem + '.parquet'
        path_save = path_save / file_save
        
        if path_save.exists():
            logger.info(f"-- -- Acronym Detection and Expansion output already exists at {path_save.as_posix()}")
        else:
            logger.info(f"-- -- Acronym Detection and Expansion output does not exist at {path_save.as_posix()}")
            logger.info(f"-- -- Running Acronym Detection and Expansion...")
            time_start = time.time()
        
            #############
            # DETECTION #
            #############
            # Decide whether to train based on configuration file and arguments
            do_train = getattr(args, 'do_train', False) or config['acr'].get('train_all_modules', False)

            # Verify if the model should be trained
            should_train = do_train or "HermesAcronymDetector" in (config['acr'].get('modules_to_train') or [])
            
            detector = HermesAcronymDetector(
                model_type=args.llm_type,
                do_train=should_train,
                data_path=args.data_path,
                logger=logger
            )
            
            #############
            # EXPANSION #
            #############
            # Verify if the model should be trained
            should_train = do_train or "HermesAcronymExpander" in (config['acr'].get('modules_to_train') or [])
            expander = HermesAcronymExpander(
                model_type=args.llm_type,
                do_train=should_train,
                data_path=args.data_path,
                logger=logger
            )
            
            # Process the DataFrame for detection and expansion
            try:
                df_out = process_dataframe(
                    path=args.data_path,
                    config=config['acr'],
                    action="complete",
                    acronym_detector=detector.module if detector else None,
                    acronym_expander=expander.module if expander else None,
                    context_window=args.context_window,
                    max_windows=args.max_windows,
                    window_overlap=args.window_overlap,
                    logger=logger
                )
            except Exception as e:
                logger.error(f"-- -- Error occured processing dataframe: {str(e)}")
                raise e
            
            logger.info(f"-- -- Acronym Detection and Expansion done in {(time.time() - time_start)/60} minutes. Saving output...")
            df_out.to_parquet(path_save, index=False)
            logger.info(f"-- -- 1. Acronym Detection and Expansion output saved to {path_save.as_posix()}")  
            
        #***********************************************************************
        # 2. Preprocessing
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 2. Preprocessing ###")
        logger.info("#"*80)
        
        source_path = path_save # Path to the output file from the previous step
        path_save = path_root_save / '2.preprocessing'
        if not path_save.exists():
            path_save.mkdir(parents=True)
        file_save = pathlib.Path(args.data_path).stem + '.parquet'    
        path_save = path_save / file_save
        
        if path_save.exists():
            logger.info(f"-- -- Preprocessing output already exists at {path_save.as_posix()}")
        else:
            logger.info(f"-- -- Preprocessing output does not exist at {path_save.as_posix()}")
            logger.info(f"-- -- Running Preprocessing...")
            time_start = time.time()
        
            cmd = [
                "python", (config['preproc'].get('preprocessing_script')),
                "--source_path", source_path.as_posix(),
                "--source_type", (config['preproc'].get('source_type')),
                "--source", args.preproc_source,
                "--destination_path", path_save.as_posix(),
                "--lang", (config['preproc'].get('lang')),
                "--spacy_model", (config['preproc'].get('spacy_model')),
                "--do_embeddings"
            ]
            
            try:
                logger.info(f'-- -- Running preprocessing command {" ".join(cmd)}')
                subprocess.check_output(cmd)
            except subprocess.CalledProcessError as e:
                logger.info('-- -- Preprocessing failed. Revise command')
                logger.info(e.output)
            
            logger.info(f"-- -- Preprocessing done in {(time.time() - time_start)/60} minutes. Saving output...")
            logger.info(f"-- -- 2. Preprocessing output saved to {path_save.as_posix()}")
            
        #***********************************************************************
        # 3. Equivalence Detection
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 3. Equivalence Detection ###")
        logger.info("#"*80)
        
        load_data_path = path_save.parent / (path_save.stem + "_embeddings.parquet") # Path to the output file from the previous step
        path_save = path_root_save / '3.equivalence_detection'
        if not path_save.exists():
            path_save.mkdir(parents=True)
            
        file_save = pathlib.Path(args.data_path).stem + '.json'
        file_save_copy = pathlib.Path(args.data_path).stem + '_copy.json'
        path_save_eqs = pathlib.Path(config["equiv"].get("path_save")) 
        path_save_copy = path_save / file_save_copy
 
        if path_save_copy.exists():
            logger.info(f"-- -- Equivalences output already exists at {path_save_copy}")
        else:
            logger.info(f"-- -- Equivalences output does not exist at {path_save_copy}")
            logger.info(f"-- -- Running Equivalence Detection...")
            time_start = time.time()    
            
            if not path_save_eqs.exists():
                logger.info(f"-- -- Creating directory {path_save_eqs}")
                path_save_eqs.mkdir(parents=True)
            else:
                logger.info(f"-- -- Directory {path_save_eqs} already exists")
                logger.info(f"-- -- Deleting files in {path_save_eqs} from previous runs...")
                # delete the files in the directory
                for file in path_save_eqs.iterdir():
                    if file.is_file():
                        file.unlink()
            
            path_save_eqs = path_save_eqs / file_save
            
            eq_generator = HermesEquivalencesGenerator(
                use_optimized = True,
                do_train = True,
            )
        
            # Train auxiliary topic model
            model_path = path_save / pathlib.Path(args.data_path).stem /'aux_topic_model'
            if model_path.exists():
                logger.info(f"-- -- Auxiliary topic model already exists at {model_path}")
            else:
                model_path.mkdir(parents=True)
                logger.info(f"-- -- Auxiliary topic model does not exist at {model_path}")
                logger.info(f"-- -- Training auxiliary topic model...")

                this_args = argparse.Namespace(
                    **{k: v for k, v in vars(args).items() 
                    if v is not None and k in ["further_proc", "sample", "num_iters"]})

                # Assign new values to the copied Namespace object
                this_args.model_path = model_path.as_posix()
                this_args.load_data_path = load_data_path.as_posix()
                this_args.num_topics = config['equiv']['num_topics']
                                
                model = train_model(
                    model_path = model_path.as_posix(),
                    model_type = config['equiv']['model_type'],
                    num_topics = config['equiv']['num_topics'],
                    further_proc = config['equiv']['further_proc'],
                    logger = logger,
                    env = pathlib.Path(config['llm']['env']),
                    args = this_args
                )
                topics = model.print_topics()
                print(f"-- -- Topics from auxiliary trained model: {topics}")
                for i, topic in enumerate(topics):
                    print("Topic #", i)
                    print(topics[topic])
            
            path_to_source = model_path / f"{config['equiv']['model_type']}_{config['equiv']['num_topics']}" / "vocabulary.txt" if args.source_eq == "vocabulary" else model_path
    
            logger.info(f"-- -- Generating equivalences from {args.source_eq}...")
            eq_generator.generate_equivalences(
                source = args.source_eq,
                path_to_source = path_to_source,
                path_save = path_save_eqs,
                model_type = config['equiv']['model_type'],
                language = config['equiv']['language'],
            )
        
            # Copy the generated file to output so it is not overwritten when multiple runs are executed
            logger.info(f"-- -- Equivalences saved to {path_save_eqs}")
            shutil.copy(path_save_eqs, path_save_copy)
        
        #**********************************************************************
        # 4. Training
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 4. Training ###")
        logger.info("#"*80)
        
        path_save = path_root_save / '4.training'
        if not path_save.exists():
            path_save.mkdir(parents=True)
            
        model_path = path_save / pathlib.Path(args.data_path).stem
        
        if model_path.exists():
            logger.info(f"-- -- Model output already exists at {model_path}")
        else:
            model_path.mkdir(parents=True)
            logger.info(f"-- -- Model output does not exist at {model_path}. Training model...")
            
            this_args = argparse.Namespace(
                **{k: v for k, v in vars(args).items() 
                if v is not None and k in ["further_proc", "sample", "num_iters"]})

            # Assign new values to the copied Namespace object
            this_args.model_path = model_path.as_posix()
            this_args.load_data_path = load_data_path.as_posix()
            this_args.num_topics = args.num_topics
            
            if args.model_type == 'all':
                logger.info( "-- -- Training all models...")
                models = ['MalletLda', 'Ctm', 'BERTopic', 'TopicGPT']
            else:
                logger.info( f"-- -- Training model of type {args.model_type}...")
                models = [args.model_type]
            
            for model_type in models:
                
                if model_type == 'Ctm':
                    this_args.num_iters = 50
                    logger.info(f"-- -- Training model with {this_args.num_iters} iterationss because it is a Ctm model...")
                else:
                    this_args.num_iters = args.num_iters
                    
                model = train_model(
                    model_path = model_path.as_posix(),
                    model_type = model_type,
                    num_topics = args.num_topics,
                    further_proc = args.further_proc,
                    logger = logger,
                    env = pathlib.Path(config['llm']['env']),
                    args = this_args
                )
                topics = model.print_topics()
                print(f"-- -- Topics from auxiliary trained model: {topics}")
                for i, topic in enumerate(topics):
                    print("Topic #", i)
                    print(topics[topic])
    
    else:
        logger.info("-- -- Running HERMES pipeline in non-optimized mode...")
        
        path_root_save = pathlib.Path(args.save_path) / 'non_optimized'
        if not path_root_save.exists():
            path_root_save.mkdir(parents=True)
            
        #***********************************************************************
        # 1. Preprocessing
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 2. Preprocessing ###")
        logger.info("#"*80)
        
        source_path = args.data_path
        path_save = path_root_save / '2.preprocessing'
        if not path_save.exists():
            path_save.mkdir(parents=True)
        file_save = pathlib.Path(args.data_path).stem + '.parquet'    
        path_save = path_save / file_save
        
        if path_save.exists():
            logger.info(f"-- -- Preprocessing output already exists at {path_save.as_posix()}")
        else:
            logger.info(f"-- -- Preprocessing output does not exist at {path_save.as_posix()}")
            logger.info(f"-- -- Running Preprocessing...")
            time_start = time.time()
        
            cmd = [
                "python", (config['preproc'].get('preprocessing_script')),
                "--source_path", source_path,
                "--source_type", (config['preproc'].get('source_type')),
                "--source", args.preproc_source + "_non_optimized",
                "--destination_path", path_save.as_posix(),
                "--lang", (config['preproc'].get('lang')),
                "--spacy_model", (config['preproc'].get('spacy_model')),
                "--do_embeddings"
            ]
            
            try:
                logger.info(f'-- -- Running preprocessing command {" ".join(cmd)}')
                subprocess.check_output(cmd)
            except subprocess.CalledProcessError as e:
                logger.info('-- -- Preprocessing failed. Revise command')
                logger.info(e.output)
            
            logger.info(f"-- -- Preprocessing done in {(time.time() - time_start)/60} minutes. Saving output...")
            logger.info(f"-- -- 2. Preprocessing output saved to {path_save.as_posix()}")
    
        #**********************************************************************
        # 4. Training
        #***********************************************************************
        logger.info("#"*80)
        logger.info(f"### 4. Training ###")
        logger.info("#"*80)
        
        load_data_path = path_save.parent / (path_save.stem + "_embeddings.parquet") # Path to the output file from the previous step
        
        path_save = path_root_save / '4.training'
        if not path_save.exists():
            path_save.mkdir(parents=True)
            
        model_path = path_save / pathlib.Path(args.data_path).stem
        
        if model_path.exists():
            logger.info(f"-- -- Model output already exists at {model_path}")
        else:
            model_path.mkdir(parents=True)
            logger.info(f"-- -- Model output does not exist at {model_path}. Training model...")
            
            this_args = argparse.Namespace(
                **{k: v for k, v in vars(args).items() 
                if v is not None and k in ["sample", "num_iters"]})

            # Assign new values to the copied Namespace object
            this_args.model_path = model_path.as_posix()
            this_args.load_data_path = load_data_path.as_posix()
            this_args.further_proc = False
            this_args.num_topics = args.num_topics
            
            if args.model_type == 'all':
                logger.info( "-- -- Training all models...")
                models = ['MalletLda', 'Ctm', 'BERTopic', 'TopicGPT']
            else:
                logger.info( f"-- -- Training model of type {args.model_type}...")
                models = [args.model_type]
            
            for model_type in models:
                if model_type == 'Ctm':
                    this_args.num_iters = 50
                    logger.info(f"-- -- Training model with {this_args.num_iters} iterationss because it is a Ctm model...")
                else:
                    this_args.num_iters = args.num_iters
                    
                model = train_model(
                    model_path = model_path.as_posix(),
                    model_type = model_type,
                    num_topics = args.num_topics,
                    further_proc = False,
                    logger = logger,
                    env = pathlib.Path(config['llm']['env']),
                    args = this_args
                )
                topics = model.print_topics()
                print(f"-- -- Topics from auxiliary trained model: {topics}")
                for i, topic in enumerate(topics):
                    print("Topic #", i)
                    print(topics[topic])

if __name__ == "__main__":
    main()